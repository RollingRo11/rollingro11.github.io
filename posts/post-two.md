---
title: "Getting started in Mechanistic Interpretability"
date: "2025-06-16"
excerpt: "A comprehensive list of resources to get started!"
---


# Resources
*(Mostly adapted from [this blog post](https://transformer-circuits.pub/2025/april-update/index.html#work) from Anthropic! I've tuned it to be more beginner friendly.)*



If you're here, you know that Mechanistic Interpretability is a field at the cutting edge of AI research - specifically focused on understanding how models work under the microscope and eventually using our findings to make AI safer, more controllable. From a fellow beginner in the field who spent hours trying to find out where to start, this is a list of resources I think are super helpful!

## Links
- [TransformerLens](https://transformerlensorg.github.io/TransformerLens/content/getting_started_mech_interp.html): An open-source library for transformer interpretability
- [NNSight](https://nnsight.net/): An open source interpretability repository that works with ndif (this is run by Professor Bau from Northeastern!)
- [ARENA](https://github.com/callummcdougall/ARENA_3.0): An open source tutorial for learning about key concepts and techniques
- [MATS Mini Project](https://docs.google.com/spreadsheets/d/1D7i3DUONcuFIPAYxBYjY7hvzhh4-YgL1diBgc6-h4pI/edit?gid=0#gid=0): A list of mini-projects from leaders in the field
- [EleutherAI](https://www.eleuther.ai/): A place to discuss and contribute to open-source LLM research
- [ML Collective](https://mlcollective.org/): A non-profit to collaborate on interpretability research projects
- [Transluce](https://transluce.org/): A research lab with an open-source neuron observability interface and other tools
- [Alignment Forum](https://www.alignmentforum.org/): A forum for discussions about AI safety and interpretability
- [LessWrong](https://www.lesswrong.com/): A forum for discussions and open source research about interpretability


## General Guide
-  When you’re ready to run your first experiments, try using an open source project (Neuronpedia, NNSight, TransformerLens, or Transluce).
- Develop a deep understanding of the state of interpretability research, not unlike your prior fields of research. Including reading some of the foundational papers in the field.
- Familiarize yourself with large codebases in interpretability that multiple people or teams are contributing to (Neuronpedia, NNSight, TransformerLens).
  - Work in and around a modern codebase, ideally one that gives you exposure to key general development concepts like testing, types, and CI/CD. This could be by contributing to an open-source repository (ideally one with “good first issue” tags), a well maintained academic software project, or signing up for a hackathon. Find a problem to work on that requires you to do some data visualization or use distributed compute.
